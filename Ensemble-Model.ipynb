{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "111865a2-3f3f-4fe5-92c6-c9c6df18196d"
    }
   },
   "source": [
    "# TAAR â€“ Ensemble Learning\n",
    "\n",
    "Currently we have four different addon recommenders in TAAR. The goal of this project is to combine them into one model. In Machine Learning, this is known as an ensemble model. Generally, ensembles work well if the individual models are quite diverse. There is a good chance that this is the case here as our individual models are all based on different features.\n",
    "\n",
    "The method that we're trying to implement here is known as *linear stacking* or *linear blending*. Our ensemble model uses the output of the previous models as features and learns to weight them, i.e. it learns how useful the individual recommenders are in general. To do this, we have to extend the existing recommenders so that they're able to return weighted recommendations (meaning pairs of recommendations and confidence scores, instead of just an ordered list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.addPyFile(\"./taar/dist/mozilla_taar-0.0.16.dev15+g824aa58.d20171018-py2.7.egg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1b8ecc9d-bae3-4e3e-9319-87cc7c40e6cd"
    }
   },
   "source": [
    "## Collecting and preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "efb2bd5b-f229-472e-b482-d0716db9df39"
    }
   },
   "source": [
    "### Retrieving the relevant variables from the longitudinal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e493852b-cc59-4050-88cd-3dd073cf22ba"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 ms, sys: 4 ms, total: 24 ms\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "frame = sqlContext.sql(\"\"\"\n",
    "WITH valid_clients AS (\n",
    "    SELECT *\n",
    "    FROM longitudinal\n",
    "    WHERE normalized_channel='release' AND build IS NOT NULL AND build[0].application_name='Firefox'\n",
    "),\n",
    "\n",
    "addons AS (\n",
    "    SELECT client_id, feature_row.*\n",
    "    FROM valid_clients\n",
    "    LATERAL VIEW explode(active_addons[0]) feature_row\n",
    "),\n",
    "    \n",
    "non_system_addons AS(\n",
    "    SELECT client_id, collect_set(key) AS installed_addons\n",
    "    FROM addons\n",
    "    WHERE NOT value.is_system\n",
    "    GROUP BY client_id\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    l.client_id,\n",
    "    non_system_addons.installed_addons,\n",
    "    settings[0].locale AS locale,\n",
    "    geo_city[0] AS geo_city,\n",
    "    subsession_length[0] AS subsession_length,\n",
    "    system_os[0].name AS os,\n",
    "    scalar_parent_browser_engagement_total_uri_count[0].value AS total_uri,\n",
    "    scalar_parent_browser_engagement_tab_open_event_count[0].value as tab_open_count,\n",
    "    places_bookmarks_count[0].sum as bookmark_count,\n",
    "    scalar_parent_browser_engagement_unique_domains_count[0].value as unique_tlds,\n",
    "    profile_creation_date[0] as profile_date,\n",
    "    submission_date[0] as submission_date\n",
    "FROM valid_clients l LEFT OUTER JOIN non_system_addons\n",
    "ON l.client_id = non_system_addons.client_id\n",
    "\"\"\")\n",
    "\n",
    "rdd = frame.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, Spark doesn't correctly adjust the number of partitions in our RDD here. This makes it extremely slow to process the data because we're only using one core instead of `number of cores`  (typically 16 here) times `number of nodes`. Because of this, we manually repartition the RDD here. This will dramatically speed things up when having a cluster with multiple nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2166"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = frame.rdd.repartition(sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ce033f1e-e6a6-4b41-aacf-dfc3cdaf3b47"
    }
   },
   "source": [
    "### Loading addon data (AMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "7ee9e42a-8e97-4e88-85d4-990ccedb68b2"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "AMO_DUMP_BUCKET = 'telemetry-parquet'\n",
    "AMO_DUMP_KEY = 'telemetry-ml/addon_recommender/addons_database.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "74908378-57ea-4eeb-8f77-75f7b7324173"
    }
   },
   "outputs": [],
   "source": [
    "def load_amo_external_whitelist():\n",
    "    \"\"\" Download and parse the AMO add-on whitelist.\n",
    "    :raises RuntimeError: the AMO whitelist file cannot be downloaded or contains\n",
    "                          no valid add-ons.\n",
    "    \"\"\"\n",
    "    final_whitelist = []\n",
    "    amo_dump = {}\n",
    "    try:\n",
    "        # Load the most current AMO dump JSON resource.\n",
    "        s3 = boto3.client('s3')\n",
    "        s3_contents = s3.get_object(Bucket=AMO_DUMP_BUCKET, Key=AMO_DUMP_KEY)\n",
    "        amo_dump = json.loads(s3_contents['Body'].read())\n",
    "    except ClientError:\n",
    "        logger.exception(\"Failed to download from S3\", extra={\n",
    "            \"bucket\": AMO_DUMP_BUCKET,\n",
    "            \"key\": AMO_DUMP_KEY})\n",
    "\n",
    "    # If the load fails, we will have an empty whitelist, this may be problematic.\n",
    "    for key, value in amo_dump.items():\n",
    "        addon_files = value.get('current_version', {}).get('files', {})\n",
    "        # If any of the addon files are web_extensions compatible, it can be recommended.\n",
    "        if any([f.get(\"is_webextension\", False) for f in addon_files]):\n",
    "            final_whitelist.append(value['guid'])\n",
    "\n",
    "    if len(final_whitelist) == 0:\n",
    "        raise RuntimeError(\"Empty AMO whitelist detected\")\n",
    "\n",
    "    return final_whitelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "55539eaa-011f-422b-b92a-ec4b1fc9c9ca"
    }
   },
   "source": [
    "### Filtering out legacy addons\n",
    "\n",
    "The collaborative recommender should make recommendations based on WebExtensions. In general, our ensemble should also only recommend WebExtensions. Because of this, we need a way to filter for WebExtensions. We store this collection in two different data structures (list, set) as both will be useful in the remainder of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): 169.254.169.254\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): 169.254.169.254\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): s3-us-west-2.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "whitelist = load_amo_external_whitelist()\n",
    "whiteset = set(whitelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "ed2396bb-3f2e-4fa6-9388-a3d9e34b97fb"
    }
   },
   "outputs": [],
   "source": [
    "def get_whitelisted_addons(installed_addons):\n",
    "    return whiteset.intersection(installed_addons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "003cb37b-c00a-40d3-b512-fbe47aa8e09f"
    }
   },
   "source": [
    "### Completing client data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f0c13767-4a27-4202-87cb-0babe8b5fbbd"
    }
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse as parse_date\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "ad65dbfc-2f3c-4b79-a726-68d70339715c"
    }
   },
   "outputs": [],
   "source": [
    "def compute_weeks_ago(formatted_date):\n",
    "    try:\n",
    "        date = parse_date(formatted_date).replace(tzinfo=None)\n",
    "    except ValueError: # raised when the date is in an unknown format\n",
    "        return float(\"inf\")\n",
    "    \n",
    "    days_ago = (datetime.today() - date).days\n",
    "    return days_ago / 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "95f8c02e-2d12-4c24-9997-1b17ed78ff2e"
    }
   },
   "outputs": [],
   "source": [
    "def complete_client_data(client_data):\n",
    "    client = client_data.asDict()\n",
    "    \n",
    "    addons = client['installed_addons'] or []\n",
    "    client['installed_addons'] = get_whitelisted_addons(addons)\n",
    "    client['disabled_addons_ids'] = addons\n",
    "    \n",
    "    client['locale'] = str(client['locale'])\n",
    "    client['profile_age_in_weeks'] = compute_weeks_ago(client['profile_date'])\n",
    "    client['submission_age_in_weeks'] = compute_weeks_ago(client['submission_date'])\n",
    "    \n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "17eece00-9e0c-4231-80d5-a153d589d650"
    }
   },
   "outputs": [],
   "source": [
    "completed_rdd = rdd.map(complete_client_data).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "05cbfe08-2f2d-417b-af43-cb3f406c7aaa"
    }
   },
   "source": [
    "## Computing confidence scores\n",
    "\n",
    "We add a method `get_weighted_recommendations` to each recommender. This method returns a (default) dictionary where the keys are addons and the values indicate how confident the recommender is that the respective addon would be a good recommendation.\n",
    "\n",
    "A default dictionary is used to return `0 ` if a recommender is not in a position to judge how good a potential recommendation would be. The scores returned by this method do not need to be normalized. This is done one step afterwards.\n",
    "\n",
    "**Important note**: The code below is not directly used in the rest of the notebook, it's just here for explanatory and documentation reasons. To use the adapted classes on the worker nodes, they need to be in the TAAR egg, because pickle seems to have problems with subclassing here. Thus, the code below is copied into the TAAR folder from where the TAAR egg is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "67eaa22a-fd0e-4955-a2a6-85dba418bc70"
    }
   },
   "source": [
    "### Collaborative Recommender\n",
    "\n",
    "Use the scores computed internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from taar.recommenders import CollaborativeRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def java_string_hashcode(s):\n",
    "    h = 0\n",
    "    for c in s:\n",
    "        h = (31 * h + ord(c)) & 0xFFFFFFFF\n",
    "    return ((h + 0x80000000) & 0xFFFFFFFF) - 0x80000000\n",
    "\n",
    "\n",
    "def positive_hash(s):\n",
    "    return java_string_hashcode(s) & 0x7FFFFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NewCollaborativeRecommender(CollaborativeRecommender):\n",
    "    def recommend(self, client_data, limit):\n",
    "        recommendations = self.get_weighted_recommendations(client_data)\n",
    "        \n",
    "        # Sort the suggested addons by their score and return the sorted list of addon\n",
    "        # ids.\n",
    "        sorted_dists = sorted(recommendations.items(), key=op.itemgetter(1), reverse=True)\n",
    "        return [s[0] for s in sorted_dists[:limit]]\n",
    "    \n",
    "    def get_weighted_recommendations(self, client_data):\n",
    "        # Addons identifiers are stored as positive hash values within the model.\n",
    "        installed_addons =\\\n",
    "            [positive_hash(addon_id) for addon_id in client_data.get('installed_addons', [])]\n",
    "\n",
    "        # Build the query vector by setting the position of the queried addons to 1.0\n",
    "        # and the other to 0.0.\n",
    "        query_vector = np.array([1.0 if (entry.get(\"id\") in installed_addons) else 0.0\n",
    "                                 for entry in self.raw_item_matrix])\n",
    "\n",
    "        # Build the user factors matrix.\n",
    "        user_factors = np.matmul(query_vector, self.model)\n",
    "        user_factors_transposed = np.transpose(user_factors)\n",
    "\n",
    "        # Compute the distance between the user and all the addons in the latent\n",
    "        # space.\n",
    "        distances = {}\n",
    "        for addon in self.raw_item_matrix:\n",
    "            # We don't really need to show the items we requested. They will always\n",
    "            # end up with the greatest score. Also filter out legacy addons from the\n",
    "            # suggestions.\n",
    "            hashed_id = str(addon.get(\"id\"))\n",
    "            if (hashed_id in installed_addons or\n",
    "                    hashed_id not in self.addon_mapping or\n",
    "                    self.addon_mapping[hashed_id].get(\"isWebextension\", False) is False):\n",
    "                continue\n",
    "\n",
    "            dist = np.dot(user_factors_transposed, addon.get('features'))\n",
    "            # Read the addon ids from the \"addon_mapping\" looking it\n",
    "            # up by 'id' (which is an hashed value).\n",
    "            addon_id = self.addon_mapping[hashed_id].get(\"id\")\n",
    "            distances[addon_id] = dist\n",
    "\n",
    "        return defaultdict(int, distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bbda38c7-42d2-451a-80c2-36786da8ce04"
    }
   },
   "source": [
    "### Similarity Recommender\n",
    "\n",
    "Use similarity scores computed internally\n",
    "\n",
    "TODO: Compute logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from taar.recommenders.similarity_recommender import SimilarityRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import hamming, canberra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cdist(dist, A, b):\n",
    "    return np.array([dist(a, b) for a in A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES = [\"geo_city\", \"locale\", \"os\"]\n",
    "CONTINUOUS_FEATURES = [\"subsession_length\", \"bookmark_count\", \"tab_open_count\", \"total_uri\", \"unique_tlds\"]\n",
    "\n",
    "class NewSimilarityRecommender(SimilarityRecommender):\n",
    "    def get_similar_donors(self, client_data):\n",
    "        \"\"\"Computes a set of :float: similarity scores between a client and a set of candidate\n",
    "        donors for which comparable variables have been measured.\n",
    "        A custom similarity metric is defined in this function that combines the Hamming distance\n",
    "        for categorical variables with the Canberra distance for continuous variables into a\n",
    "        univariate similarity metric between the client and a set of candidate donors loaded during\n",
    "        init.\n",
    "        :param client_data: a client data payload including a subset fo telemetry fields.\n",
    "        :return: the sorted approximate likelihood ratio (np.array) corresponding to the\n",
    "                 internally computed similarity score and a list of indices that link\n",
    "                 each LR score with the related donor in the |self.donors_pool|.\n",
    "        \"\"\"\n",
    "        client_categorical_feats = [client_data.get(specified_key) for specified_key in CATEGORICAL_FEATURES]\n",
    "        client_continuous_feats = [client_data.get(specified_key) for specified_key in CONTINUOUS_FEATURES]\n",
    "\n",
    "        # Compute the distances between the user and the cached continuous\n",
    "        # and categorical features.\n",
    "        cont_features = cdist(canberra, self.continuous_features, client_continuous_feats)\n",
    "        \n",
    "        # The lambda trick is needed to prevent |cdist| from force-casting the\n",
    "        # string features to double.\n",
    "        cat_features = cdist(hamming, self.categorical_features, client_categorical_feats)\n",
    "\n",
    "        # Take the product of similarities to attain a univariate similarity score.\n",
    "        # Addition of 0.001 to the continuous features avoids a zero value from the\n",
    "        # categorical variables, allowing categorical features precedence.\n",
    "        distances = (cont_features + 0.001) * cat_features\n",
    "\n",
    "        # Compute the LR based on precomputed distributions that relate the score\n",
    "        # to a probability of providing good addon recommendations.\n",
    "        lrs_from_scores =\\\n",
    "            np.array([self.get_lr(distances[i]) for i in range(self.num_donors)])\n",
    "\n",
    "        # Sort the LR values (descending) and return the sorted values together with\n",
    "        # the original indices.\n",
    "        indices = (-lrs_from_scores).argsort()\n",
    "        return lrs_from_scores[indices], indices\n",
    "\n",
    "    def get_weighted_recommendations(self, client_data):\n",
    "        recommendations = defaultdict(int)\n",
    "\n",
    "        for donor_score, donor in zip(*self.get_similar_donors(client_data)):\n",
    "            for addon in self.donors_pool[donor]['active_addons']:\n",
    "                recommendations[addon] += donor_score\n",
    "        \n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ffb63f83-c518-4e50-97cf-28baaf908c5e"
    }
   },
   "source": [
    "### Locale Recommender\n",
    "\n",
    "Depends on number of installs in that locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from taar.recommenders import LocaleRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOP_ADDONS_BY_LOCALE_FILE_PATH = \"top_addons_by_locale.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NewLocaleRecommender(LocaleRecommender):\n",
    "    def __init__(self, TOP_ADDONS_BY_LOCALE_FILE_PATH):\n",
    "        OriginalLocaleRecommender.__init__(self)\n",
    "        \n",
    "        with open(TOP_ADDONS_BY_LOCALE_FILE_PATH) as data_file:\n",
    "            top_addons_by_locale = json.load(data_file)\n",
    "            \n",
    "        self.top_addons_by_locale = defaultdict(lambda: defaultdict(int), top_addons_by_locale)\n",
    "        \n",
    "    def get_weighted_recommendations(self, client_data):\n",
    "        client_locale = client_data.get('locale', None)\n",
    "        return defaultdict(int, self.top_addons_by_locale[client_locale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "450298ce-5b2e-4519-a342-a469605b08a1"
    }
   },
   "source": [
    "### Legacy Recommender\n",
    "\n",
    "1 for all replacement addons, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from taar.recommenders import LegacyRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NewLegacyRecommender(LegacyRecommender):\n",
    "    def get_weighted_recommendations(self, client_data):\n",
    "        recommendations = defaultdict(int)\n",
    "        addons = client_data.get('disabled_addons_ids', [])\n",
    "        \n",
    "        for addon in addons:\n",
    "            for replacement in self.legacy_replacements.get(addon, []):\n",
    "                recommendations[replacement] += 1\n",
    "                \n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing training, validation and test sets\n",
    "\n",
    "For training and validation purposes, only clients that have WebExtension addons installed are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "useful_clients = completed_rdd.filter(lambda client: len(client['installed_addons']) >= 1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435313"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_clients.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These users are useful for training and evaluating our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training, validation, test = useful_clients.randomSplit([0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll introduce a small helper function `random_partition`. It takes in an iterable `A` that should be partitioned into to new lists where the first list has a length of `k`. This partitioning is done randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_partition(A, k):\n",
    "    n = len(A)\n",
    "    A = list(A)\n",
    "    indices = set(sample(range(n), k))\n",
    "    \n",
    "    first = []\n",
    "    second = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        element = A[i]\n",
    "        \n",
    "        if i in indices:\n",
    "            first.append(element)\n",
    "        else:\n",
    "            second.append(element)\n",
    "            \n",
    "    return first, second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use this function to randomly decide on a subset of addons that we want to mask for a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_masked(addons):\n",
    "    return max(1, len(addons) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mask_addons(client):\n",
    "    addons = client['installed_addons']\n",
    "    num_mask = get_num_masked(addons)\n",
    "    \n",
    "    masked, unmasked = random_partition(addons, num_mask)\n",
    "    \n",
    "    client['installed_addons'] = unmasked\n",
    "    client['masked_addons'] = masked\n",
    "    \n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_masked = training.map(mask_addons).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the feature matrices\n",
    "\n",
    "For each user, we want a matrix that contains a row for each whitelisted addon and a column for each recommender. A cell then contains the confidence score that the respective recommender gave for the respective user and addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): s3-us-west-2.amazonaws.com\n",
      "INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): s3-us-west-2.amazonaws.com\n",
      "INFO:boto3.resources.action:Calling s3:get_object with {u'Bucket': 'telemetry-parquet', u'Key': 'taar/similarity/donors.json'}\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): s3-us-west-2.amazonaws.com\n",
      "INFO:boto3.resources.action:Calling s3:get_object with {u'Bucket': 'telemetry-parquet', u'Key': 'taar/similarity/lr_curves.json'}\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): s3-us-west-2.amazonaws.com\n",
      "INFO:boto3.resources.action:Calling s3:get_object with {u'Bucket': 'telemetry-parquet', u'Key': 'taar/locale/top10_dict.json'}\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): s3-us-west-2.amazonaws.com\n",
      "INFO:boto3.resources.action:Calling s3:get_object with {u'Bucket': 'telemetry-parquet', u'Key': 'taar/legacy/legacy_dict.json'}\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): s3-us-west-2.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "recommenders = {\n",
    "    \"collaborative\": CollaborativeRecommender(),\n",
    "    \"similarity\": SimilarityRecommender(),\n",
    "    \"locale\": LocaleRecommender(\"./top_addons_by_locale.json\"),\n",
    "    \"legacy\": LegacyRecommender()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_features(client_data):\n",
    "    recommendations = []\n",
    "    matrix = []\n",
    "    \n",
    "    for _, recommender in recommenders.items():\n",
    "        recommendations.append(recommender.get_weighted_recommendations(client_data))\n",
    "\n",
    "    for addon in whitelist:\n",
    "        matrix.append([features[addon] for features in recommendations])\n",
    "\n",
    "    return client_data, np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.3 s, sys: 20 ms, total: 1.32 s\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "X_unnormalized = training_masked.map(compute_features).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "The optimization algorithms that we use here are much more minimal than what you typically from highly optimized ML libs. Because of this, we need to take special care of properly preprocessing the data.\n",
    "\n",
    "In the following, we perform these operations:\n",
    "- [Min-Max scaling](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "- Changing the locale scores to a double square root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_feature_values = X_unnormalized.map(lambda (_, features): np.max(features, axis=0)).reduce(np.maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_locale_scores(scores):\n",
    "    return np.sqrt(np.sqrt(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_features((client, features)):\n",
    "    features = features / max_feature_values\n",
    "    features[:, 0] = preprocess_locale_scores(features[:, 0])\n",
    "    return client, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X_unnormalized.map(scale_features).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing recommendations then reduces down to a dot product. These results are then sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_recommendations(client_data, features, weights):\n",
    "    scores = features.dot(weights)\n",
    "    return client_data, np.argsort(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "83e7764e-9a65-41ce-91fa-21e8358acb20"
    }
   },
   "source": [
    "## Measuring the performance (MAP)\n",
    "\n",
    "We use the [MAP](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html) measure as an error metric for this optimization problem. The reason for this is mostly that we only have positive data, i.e. we know addons which users like, but we don't really have a lot of data about addons that users hate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def average_precision(client_data, recommendations):\n",
    "    tp = fp = 0.\n",
    "    masked = set(client_data['masked_addons'])\n",
    "    precisions = []\n",
    "    \n",
    "    for recommendation in recommendations:\n",
    "        if whitelist[recommendation] in masked:\n",
    "            tp += 1\n",
    "            precisions.append(tp / (tp + fp))\n",
    "            if tp == len(masked):\n",
    "                break\n",
    "        else:\n",
    "            fp += 1\n",
    "    \n",
    "    if len(precisions) > 0:\n",
    "        return np.mean(precisions)\n",
    "    else:\n",
    "        return 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0488d2b0-f77e-4b45-8bd7-09cf09dd6680"
    }
   },
   "source": [
    "## Training an ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find recommendations, compute the average precision (AP) and then calculate the mean of that (MAP). This produces a value between 0 and 1. We then subtract this value from 1 because SciPy looks for a function to minimize, not to maximize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost(weights):\n",
    "    return 1 - X.map(lambda (client_data, features): get_weighted_recommendations(client_data, features, weights))\\\n",
    "            .map(lambda (client_data, recommendations): average_precision(client_data, recommendations))\\\n",
    "            .mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing an initial guess\n",
    "\n",
    "There are many ways of choosing initial guesses. A constant vector of 1s seems to be a sensible prior (with properly normalized features it means that all recommenders are equally useful). However, randomly choosing initial values can also be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_initial_guess(n):\n",
    "    return np.ones(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_initial_guess(n):\n",
    "    return np.random.random(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging the optimization process\n",
    "\n",
    "SciPy is logging the optimization process to a stdout stream that Jupyter seems to ignore. Because it's extremely useful to see how the optimization process is actually going, we define a helper function that queries `cost` and then also prints the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verbose_cost(weights):\n",
    "    new_cost = cost(weights)\n",
    "    print \"New guess:\", weights, \"leads to a cost of\", new_cost\n",
    "    return new_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing\n",
    "\n",
    "The 4-element vectors in the following correspond to the recommenders in this order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['locale', 'legacy', 'collaborative', 'similarity']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommenders.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial guess: [ 0.38848417  0.88084589  0.75092367  0.55297014]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/anaconda2/lib/python2.7/site-packages/scipy/optimize/_minimize.py:400: RuntimeWarning: Method COBYLA cannot handle bounds.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New guess: [ 0.38848417  0.88084589  0.75092367  0.55297014] leads to a cost of 0.707622781302\n",
      "New guess: [ 1.38848417  0.88084589  0.75092367  0.55297014] leads to a cost of 0.656494973351\n",
      "New guess: [ 1.38848417  1.88084589  0.75092367  0.55297014] leads to a cost of 0.656422468122\n",
      "New guess: [ 1.38848417  1.88084589  1.75092367  0.55297014] leads to a cost of 0.654930943044\n",
      "New guess: [ 1.38848417  1.88084589  1.75092367  1.55297014] leads to a cost of 0.705653233305\n",
      "New guess: [ 2.09824812  1.88185242  1.77162925 -0.15116436] leads to a cost of 0.650331978164\n",
      "New guess: [ 1.77118452  1.88239279  1.78274545 -0.52919285] leads to a cost of 0.676865782411\n",
      "New guess: [ 2.09824812  2.13185216  1.77162925 -0.150807  ] leads to a cost of 0.650317222086\n",
      "New guess: [ 2.47914626  2.13193849  1.78550007  0.17280419] leads to a cost of 0.645031745\n",
      "New guess: [ 2.94162872  2.13430499  1.85173384  0.35089817] leads to a cost of 0.64766524252\n",
      "New guess: [ 2.47088222  2.13193989  2.03536151  0.17182145] leads to a cost of 0.644838888911\n",
      "New guess: [ 2.93514485  2.13431271  2.08578754  0.35046019] leads to a cost of 0.647555900808\n",
      "New guess: [ 2.2391335   2.13192224  2.03749002  0.61486546] leads to a cost of 0.648236958856\n",
      "New guess: [ 2.45361213  2.37412669  2.03962347  0.11241492] leads to a cost of 0.64485202398\n",
      "New guess: [ 2.40695018  2.06861335  2.05236846 -0.06080101] leads to a cost of 0.650097018706\n",
      "New guess: [ 2.40623863  2.15281843  2.03680255  0.27674149] leads to a cost of 0.647205125991\n",
      "New guess: [ 2.59372858  2.14491346  2.04137402  0.18996835] leads to a cost of 0.645074520339\n",
      "New guess: [ 2.47562601  2.11723493  2.03755087  0.11130106] leads to a cost of 0.644850497035\n",
      "New guess: [ 2.46923605  2.13154248  2.06654635  0.1729171 ] leads to a cost of 0.644832189335\n",
      "New guess: [ 2.40714605  2.12598887  2.07005314  0.17573524] leads to a cost of 0.64482586746\n",
      "New guess: [ 2.41015955  2.0958185   2.06956249  0.18328436] leads to a cost of 0.644830128659\n",
      "New guess: [ 2.38800431  2.1583545   2.11032381  0.20524088] leads to a cost of 0.644802670959\n",
      "New guess: [ 2.37211408  2.18940864  2.14394918  0.24472158] leads to a cost of 0.646658348049\n",
      "New guess: [ 2.36972605  2.17175585  2.11175431  0.14701277] leads to a cost of 0.644747091203\n",
      "New guess: [ 2.35667064  2.18512938  2.11912016  0.12307511] leads to a cost of 0.644759171807\n",
      "New guess: [ 2.36559072  2.17785816  2.09805386  0.14846163] leads to a cost of 0.644752380191\n",
      "New guess: [ 2.3651368   2.18635858  2.12730558  0.16938283] leads to a cost of 0.644767646575\n",
      "New guess: [ 2.35678667  2.16337623  2.11219305  0.14952328] leads to a cost of 0.644739497067\n",
      "New guess: [ 2.35849599  2.13264723  2.10888069  0.14523409] leads to a cost of 0.644745663067\n",
      "New guess: [ 2.34904771  2.16557804  2.11683346  0.13695873] leads to a cost of 0.644755850599\n",
      "New guess: [ 2.35505048  2.16249916  2.11736807  0.16413752] leads to a cost of 0.644762459314\n",
      "New guess: [ 2.35947533  2.15697678  2.10902591  0.14784325] leads to a cost of 0.644747134976\n",
      "New guess: [ 2.35421196  2.16644045  2.10553503  0.15035233] leads to a cost of 0.644743468454\n",
      "New guess: [ 2.35629285  2.16358134  2.11400735  0.15294105] leads to a cost of 0.644745882658\n",
      "New guess: [ 2.3550488   2.16260289  2.11246568  0.14917386] leads to a cost of 0.644737559785\n",
      "New guess: [ 2.35396551  2.16384721  2.11379679  0.14589284] leads to a cost of 0.644741053821\n",
      "New guess: [ 2.35416505  2.16328499  2.11087374  0.14935869] leads to a cost of 0.644738593765\n",
      "New guess: [ 2.35534529  2.16200211  2.11198312  0.14865235] leads to a cost of 0.644741003315\n",
      "New guess: [ 2.35423792  2.16296124  2.11354626  0.15053809] leads to a cost of 0.644737942187\n",
      "New guess: [ 2.35484684  2.16323991  2.1128379   0.14856679] leads to a cost of 0.644741385146\n",
      "New guess: [ 2.35494313  2.16249202  2.11257683  0.14962398] leads to a cost of 0.644737282883\n",
      "New guess: [ 2.35474013  2.16241058  2.11264482  0.14953948] leads to a cost of 0.644739098796\n",
      "New guess: [ 2.35526431  2.16280936  2.11259375  0.14980911] leads to a cost of 0.644737279802\n",
      "New guess: [ 2.35538456  2.16270274  2.11277262  0.14976691] leads to a cost of 0.644737776965\n",
      "New guess: [ 2.35547517  2.16256051  2.11224258  0.14990243] leads to a cost of 0.644738193279\n",
      "New guess: [ 2.35490227  2.1631241   2.11259745  0.14990005] leads to a cost of 0.644737963467\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-2aaca29e9936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_initial_guess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Initial guess:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"COBYLA\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/scipy/optimize/_minimize.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    453\u001b[0m                              **options)\n\u001b[1;32m    454\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cobyla'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cobyla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'slsqp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         return _minimize_slsqp(fun, x0, args, jac, bounds,\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/scipy/optimize/cobyla.pyc\u001b[0m in \u001b[0;36m_minimize_cobyla\u001b[0;34m(fun, x0, args, constraints, rhobeg, tol, iprint, maxiter, disp, catol, **unknown_options)\u001b[0m\n\u001b[1;32m    256\u001b[0m     xopt, info = _cobyla.minimize(calcfc, m=m, x=np.copy(x0), rhobeg=rhobeg,\n\u001b[1;32m    257\u001b[0m                                   \u001b[0mrhoend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrhoend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miprint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxfun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                                   dinfo=info)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcatol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/site-packages/scipy/optimize/cobyla.pyc\u001b[0m in \u001b[0;36mcalcfc\u001b[0;34m(x, con)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalcfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mizip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcons_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-143-063bda5c54e1>\u001b[0m in \u001b[0;36mverbose_cost\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mverbose_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnew_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"New guess:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"leads to a cost of\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-137-24f764a1f265>\u001b[0m in \u001b[0;36mcost\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclient_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_weighted_recommendations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclient_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecommendations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maverage_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecommendations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \"\"\"\n\u001b[0;32m-> 1155\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mstats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mleft_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmergeStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStatCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \"\"\"\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda2/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_features = len(recommenders)\n",
    "x0 = get_initial_guess(num_features)\n",
    "print \"Initial guess:\", x0\n",
    "minimize(verbose_cost, x0, method=\"COBYLA\", bounds=[(0, None)] * num_features, tol=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental: Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grid_search(cost, parameter_space):\n",
    "    for parameters in parameter_space:\n",
    "        print parameters, cost(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0) 0.999317795469\n",
      "(0, 0, 0, 1) 0.781044284439\n",
      "(0, 0, 0, 2) 0.781044284439\n",
      "(0, 0, 1, 0) 0.943562693329\n",
      "(0, 0, 1, 1) 0.775298736943\n",
      "(0, 0, 1, 2) 0.777014400398\n",
      "(0, 0, 2, 0) 0.943562693329\n",
      "(0, 0, 2, 1) 0.767080292565\n",
      "(0, 0, 2, 2) 0.775298736943\n",
      "(0, 1, 0, 0) 0.995510515481\n",
      "(0, 1, 0, 1) 0.781035766119\n",
      "(0, 1, 0, 2) 0.780880352461\n",
      "(0, 1, 1, 0) 0.943214127322\n",
      "(0, 1, 1, 1) 0.775202154909\n",
      "(0, 1, 1, 2) 0.776862986849\n",
      "(0, 1, 2, 0) 0.943034455897\n",
      "(0, 1, 2, 1) 0.766951549013\n",
      "(0, 1, 2, 2) 0.775129598268\n",
      "(0, 2, 0, 0) 0.995510515481\n",
      "(0, 2, 0, 1) 0.781786764264\n",
      "(0, 2, 0, 2) 0.781035766119\n",
      "(0, 2, 1, 0) 0.943193566877\n",
      "(0, 2, 1, 1) 0.776209558464\n",
      "(0, 2, 1, 2) 0.776919897482\n",
      "(0, 2, 2, 0) 0.943214127322\n",
      "(0, 2, 2, 1) 0.76798319702\n",
      "(0, 2, 2, 2) 0.775202154909\n",
      "(1, 0, 0, 0) 0.653586710269\n",
      "(1, 0, 0, 1) 0.706249773598\n",
      "(1, 0, 0, 2) 0.713821595475\n",
      "(1, 0, 1, 0) 0.650420413684\n",
      "(1, 0, 1, 1) 0.704830022379\n",
      "(1, 0, 1, 2) 0.713027854432\n",
      "(1, 0, 2, 0) 0.650320396078\n",
      "(1, 0, 2, 1) 0.703943832142\n",
      "(1, 0, 2, 2) 0.712374380858\n",
      "(1, 1, 0, 0) 0.652823591273\n",
      "(1, 1, 0, 1) 0.705832584758\n",
      "(1, 1, 0, 2) 0.713615932023\n",
      "(1, 1, 1, 0) 0.649844424588\n",
      "(1, 1, 1, 1) 0.704397272961\n",
      "(1, 1, 1, 2) 0.712828033415\n",
      "(1, 1, 2, 0) 0.649716652369\n",
      "(1, 1, 2, 1) 0.703557325313\n",
      "(1, 1, 2, 2) 0.712172539026\n",
      "(1, 2, 0, 0) 0.653352678701\n",
      "(1, 2, 0, 1) 0.705896438837\n",
      "(1, 2, 0, 2) 0.713424691705\n",
      "(1, 2, 1, 0) 0.650397013469\n",
      "(1, 2, 1, 1) 0.704522417619\n",
      "(1, 2, 1, 2) 0.712589168725\n",
      "(1, 2, 2, 0) 0.650406273884\n",
      "(1, 2, 2, 1) 0.703674904419\n",
      "(1, 2, 2, 2) 0.711961697999\n",
      "(2, 0, 0, 0) 0.653586710269\n",
      "(2, 0, 0, 1) 0.690287976483\n",
      "(2, 0, 0, 2) 0.706249773598\n",
      "(2, 0, 1, 0) 0.651445337683\n",
      "(2, 0, 1, 1) 0.684947350383\n",
      "(2, 0, 1, 2) 0.705463845082\n",
      "(2, 0, 2, 0) 0.650420413684\n",
      "(2, 0, 2, 1) 0.680590889828\n",
      "(2, 0, 2, 2) 0.704830022379\n",
      "(2, 1, 0, 0) 0.653184699287\n",
      "(2, 1, 0, 1) 0.690123263589\n",
      "(2, 1, 0, 2) 0.706156340238\n",
      "(2, 1, 1, 0) 0.651128308542\n",
      "(2, 1, 1, 1) 0.68474123067\n",
      "(2, 1, 1, 2) 0.705355728315\n",
      "(2, 1, 2, 0) 0.650086878921\n",
      "(2, 1, 2, 1) 0.68035786005\n",
      "(2, 1, 2, 2) 0.704696794839\n",
      "(2, 2, 0, 0) 0.652823591273\n",
      "(2, 2, 0, 1) 0.689644935857\n",
      "(2, 2, 0, 2) 0.705832584758\n",
      "(2, 2, 1, 0) 0.650846311386\n",
      "(2, 2, 1, 1) 0.684340948629\n",
      "(2, 2, 1, 2) 0.705035266025\n",
      "(2, 2, 2, 0) 0.649844424588\n",
      "(2, 2, 2, 1) 0.680023236015\n",
      "(2, 2, 2, 2) 0.704397272961\n"
     ]
    }
   ],
   "source": [
    "space = product(range(3), repeat=4)\n",
    "grid_search(cost, space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to old the method\n",
    "\n",
    "To validate if the MAP numbers that we get are any good, it's useful to compare them to the results of the previous recommendation process. The following is a minimal reimplementation of this `RecommendationManager`. It's used here because we want to use our masked data instead of the data fetched from HBase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RecommendationManager:\n",
    "    def __init__(self, recommenders):\n",
    "        self.recommenders = recommenders\n",
    "        \n",
    "    def recommend(self, client_data, limit):\n",
    "        recommendations = []\n",
    "        \n",
    "        for r in self.recommenders:\n",
    "            recommendations += r.recommend(client_data, limit)\n",
    "            \n",
    "            if len(recommendations) >= limit:\n",
    "                break\n",
    "            \n",
    "        return recommendations[:limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function is similar to `map(superlist.index, sublist`) but ignores elements from the sublist that don't appear in the superlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_elements_to_indices(superlist, sublist):\n",
    "    result = []\n",
    "    \n",
    "    for a in sublist:\n",
    "        for i, b in enumerate(superlist):\n",
    "            if a == b:\n",
    "                result.append(i)\n",
    "                break\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_recommendation_manager(mngr):\n",
    "    return 1 - training_masked\\\n",
    "        .map(lambda user: (user, mngr.recommend(user, 10)))\\\n",
    "        .map(lambda (user, recommendations): average_precision(user, list_elements_to_indices(whitelist, recommendations)))\\\n",
    "        .mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the previous recommendation manager performs much worse than our new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94540781980452748"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mngr = RecommendationManager([recommenders[\"legacy\"], recommenders[\"collaborative\"], recommenders[\"similarity\"], recommenders[\"locale\"]])\n",
    "evaluate_recommendation_manager(mngr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this comparison is a little bit unfair. The locale recommender is generally extremely useful and can be used as a better baseline. With this ordering (where nearly only the locale recommender is queried), we get a much more comparable result. The results are now in the same ballpark and the ensemble is better by around 2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66339567712705083"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mngr = RecommendationManager([recommenders[\"locale\"], recommenders[\"legacy\"], recommenders[\"collaborative\"], recommenders[\"similarity\"]])\n",
    "evaluate_recommendation_manager(mngr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66444559195452979"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mngr = RecommendationManager([recommenders[\"locale\"]])\n",
    "evaluate_recommendation_manager(mngr)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {
    "2f1f2259-13cc-419e-9ea1-dbb006a73fd3": {
     "id": "2f1f2259-13cc-419e-9ea1-dbb006a73fd3",
     "layout": "grid",
     "prev": null,
     "regions": {
      "14e63bca-e053-4d62-b454-0ccccfdb2877": {
       "attrs": {
        "height": 0.6666666666666666,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 0.5,
        "x": 0.5,
        "y": 0
       },
       "id": "14e63bca-e053-4d62-b454-0ccccfdb2877"
      },
      "15eef230-77dc-454f-958b-4a7d29a1b623": {
       "attrs": {
        "height": 0.3333333333333333,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 1,
        "x": 0,
        "y": 0.6666666666666666
       },
       "id": "15eef230-77dc-454f-958b-4a7d29a1b623"
      },
      "4bb60b22-d190-45a3-91fa-98bb62418f86": {
       "attrs": {
        "height": 0.6666666666666666,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 0.5,
        "x": 0.25,
        "y": 0
       },
       "id": "4bb60b22-d190-45a3-91fa-98bb62418f86"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "1249px",
    "left": "0px",
    "right": "1228.33px",
    "top": "107px",
    "width": "478px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
